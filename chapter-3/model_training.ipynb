{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter-3: Create and train a ML Segmentation Model using AWS Sagemaker\n",
    "\n",
    "The objectives you complete during the course of this chapter introduce you to the process of implementing the SageMaker model training tool. You will engage in this process by completing the following objectives:\n",
    "\n",
    "### AWS SageMaker:\n",
    "Amazon SageMaker helps data scientists and developers to prepare, build, train, and deploy high-quality machine learning (ML) models quickly by bringing together a broad set of capabilities purpose-built for ML.\n",
    "\n",
    "Simply put, It is a set of cloud based (specifically, AWS) apps that focus on labeling, training, testing and deploying models.\n",
    "\n",
    "## How it Works in context of our HLD problem:\n",
    "\n",
    "### Prepare:\n",
    "\n",
    "Sagemaker provides in-house labelling tools and data wrangling tools for some common ML workflows. Owing to the spatio temporal nature of the HLD dataset, we will be skipping this functionality of sagemaker and use IMPACT-Built ImageLabeler tool for identifying features and labeling them. This workflow has been covered in Chapter-0 and Chapter-1 of this workshop\n",
    "\n",
    "### Build, Train & Tune:\n",
    "\n",
    "Sagemaker provides access to cloud-hosted jupyter notebooks along with pre-built ML models. For our purposes, The model we are using for this demo is Unet segmentation model (https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/). The architecture is a stack of convolutions followed by de-convolutions. This model assigns a class label to each pixel of the input and gives a output matching the size of the input. The resulting output, once trained with high-latitude dust(HLD) masks, will segment any given image to HLD and non-HLD pixels. we will be covering the process in this chapter (chapter-3).\n",
    "\n",
    "### Deploy and Manage:\n",
    "\n",
    "Sagemaker provides endpoints to infer from the trained models. This functionality will be showcased in Chapter-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r src/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import fiona\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import rasterio.features\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "from IPython.display import Image as Display\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Chapter-2: setup access, download, and process data into ML ready format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_NUMBER = \"350996086543\"\n",
    "ROLE_NAME = \"notebookAccessRole\"\n",
    "ROLE_ARN = f\"arn:aws:iam::{ACCOUNT_NUMBER}:role/{ROLE_NAME}\"\n",
    "SOURCE_BUCKET = \"impact-datashare\"\n",
    "DESTINATION_BUCKET = f\"s3://{ACCOUNT_NUMBER}-model-bucket\"\n",
    "\n",
    "# NOTE: Use image_url function above to create a valid url, if the shapefile generation was not done in Aqua, TrueColor \n",
    "DATA_FOLDER = \"data\"\n",
    "EVENT = \"hld-labeled\"\n",
    "IMAGE_FOLDER = \"images\"\n",
    "SHAPEFILE_FOLDER = \"shapefiles\"\n",
    "LOG_FOLDER = \"logs\"\n",
    "URL = \"https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?SERVICE=WMS&REQUEST=GetMap&layers=MODIS_Aqua_CorrectedReflectance_TrueColor&version=1.3.0&crs=EPSG:4326&transparent=false&width={}&height={}&bbox={}&format=image/tiff&time={}\"\n",
    "KM_PER_DEG_AT_EQ = 111.\n",
    "RESOLUTION = 0.25\n",
    "\n",
    "def assumed_role_session():\n",
    "    # Assume the \"notebookAccessRole\" role we created using AWS CDK.\n",
    "    client = boto3.client('sts')\n",
    "    creds = client.assume_role(\n",
    "        RoleArn=ROLE_ARN,\n",
    "        RoleSessionName=ROLE_NAME\n",
    "    )['Credentials']\n",
    "    return boto3.session.Session(\n",
    "        aws_access_key_id=creds['AccessKeyId'],\n",
    "        aws_secret_access_key=creds['SecretAccessKey'],\n",
    "        aws_session_token=creds['SessionToken'],\n",
    "        region_name='us-east-1'\n",
    "    )\n",
    "\n",
    "def mkdir(foldername):\n",
    "    if os.path.exists(foldername):\n",
    "        print(f\"'{foldername}' folder already exists.\")\n",
    "        return\n",
    "    os.makedirs(foldername)\n",
    "    print(f\"Created folder: {foldername}\")\n",
    "\n",
    "    \n",
    "def delete_folder(foldername):\n",
    "    shutil.rmtree(foldername) \n",
    "    \n",
    "# Remove already existing folder for a split and create a new one with passed filenames\n",
    "def create_split(split, files):\n",
    "    \"\"\"\n",
    "    Clear and create folder with new files.\n",
    "    split: choice of \"train\", \"test\", and \"val\"\n",
    "    files: list of tiff file paths\n",
    "    \"\"\"\n",
    "    print(f'Preparing {split} split with {len(files)} examples.')\n",
    "    folder_name = f\"{DATA_FOLDER}/{split}\"\n",
    "    if os.path.exists(folder_name):\n",
    "        delete_folder(folder_name)\n",
    "    mkdir(folder_name)\n",
    "    for filename in files:\n",
    "        internal_filename = filename.split('/')[-1]\n",
    "        bitmap_filename = filename.replace('.tiff', '_bitmap.png')\n",
    "        shutil.copyfile(filename, f\"{folder_name}/{internal_filename}\")\n",
    "        shutil.copyfile(bitmap_filename, f\"{folder_name}/{bitmap_filename.split('/')[-1]}\")\n",
    "    \n",
    "def prepare_splits(source_folder, splits={'train': 0.6, 'val': 0.2, 'test': 0.2}):\n",
    "    files = glob(f\"{source_folder}/*.tiff\")\n",
    "    print(f\"Total examples found: {len(files)}\")\n",
    "    random.shuffle(files)\n",
    "    length = len(files)\n",
    "    train_limit = math.ceil(length * splits['train'])\n",
    "    val_limit = train_limit + math.ceil(length * splits['train'])\n",
    "    create_split('train', files[0:train_limit])\n",
    "    create_split('val', files[train_limit:val_limit])\n",
    "    create_split('test', files[train_limit:val_limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = assumed_role_session()  # Create a aws session with appropriate permissions\n",
    "prepare_datasets(session) # prepare ML Ready data from shapefiles labeled in Imagealabeler \n",
    "download_dataset(session) # Download preprocessed data (just in case prepare_dataset() doesn't work)\n",
    "prepare_splits(f\"{DATA_FOLDER}/{IMAGE_FOLDER}\") # split data into train/val/test chunks\n",
    "!cp data ../ # move data to parent folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a sagemaker session to upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "train_images = sagemaker_session.upload_data(path='../data/train')\n",
    "val_images = sagemaker_session.upload_data(path='../data/val')\n",
    "test_images = sagemaker_session.upload_data(path='../data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach a Tensorboard session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = sagemaker_session.boto_region_name\n",
    "tensorflow_logs_path = f'{DESTINAION_BUCKET}/{DATA_FOLDER}/{LOG_FOLDER}'\n",
    "!AWS_REGION={aws_region} tensorboard --logdir {tensorflow_logs_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensorflow wrapper provided by sagemaker to train a unet model\n",
    "\n",
    "The SageMaker Python SDK TensorFlow estimators and models and the SageMaker open-source TensorFlow containers make writing a TensorFlow script and running it in SageMaker easier.\n",
    "\n",
    "We define a sagemaker instance using `sagemaker.tensorflow.Tensorflow` class. \n",
    "\n",
    "- `entry_point` parameter should point to the underlying tensorflow model implementation.\n",
    "- `source_dir` points to the folder that contains the `entry_point`.\n",
    "- `role` should be given tthe appropriate notebook access role we've created in chaper-2\n",
    "- `instance_count`, `instance_type` defines the number of instances and the type of instance of the EC2 instance that will be used for compute.\n",
    "- `output_path` - dictates where the output files from training the model will reside\n",
    "- `image_uri` should point t the appropriate tensorflow ecr container image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "keras_metric_definition = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \".*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \".*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*\"},\n",
    "    {\n",
    "        \"Name\": \"validation:accuracy\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:loss\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"sec/steps\",\n",
    "        \"Regex\": \".* (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+\",\n",
    "    },\n",
    "]\n",
    "estimator = TensorFlow(\n",
    "    entry_point='hld_sagemaker_demo.py',\n",
    "    source_dir=\"/home/ec2-user/SageMaker/workshop_notebooks/chapter-3/src\",\n",
    "    role=ROLE_NAME,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    py_version='py3',\n",
    "    output_path=DESTINATION_BUCKET,\n",
    "    image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.4.1-gpu-py37-cu110-ubuntu18.04',\n",
    "    metric_definition=keras_metric_definition,\n",
    "    distribution={\n",
    "        'parameter_server': {'enabled': True}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Training the model is as simple as calling estimator.fit(), providing it appropriate arguments that is expected by `hld_sagemaker_demo.py`, which is the entry point for training the custom model (from the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    {\n",
    "        'train': train_images,\n",
    "        'eval': val_images, \n",
    "        'test': test_images, \n",
    "        'log_dir': f'{DATA_FOLDER}/{LOG_FOLDER}/{log_file}'\n",
    "    }, \n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.deploy(initial_instance_count=1, instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment (move to chapter-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "model = TensorFlowModel(framework_version='2.4.1', model_data=f'{DESTINATION_BUCKET}/tensorflow-training-2021-05-05-10-10-34-979/output/model.tar.gz', role=ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(np.asarray([np.zeros((256, 256, 3))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
