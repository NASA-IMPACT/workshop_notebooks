{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and understand the data in hand\n",
    "\n",
    "A subset of High Latitude Dust data is pre-prepared and uploaded into a s3 bucket (impact-datashare). The details can be found at https://github.com/nasa-impact/data_share ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r src/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import fiona\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import rasterio.features\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from io import BytesIO\n",
    "from IPython.display import Image as Display\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Constant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_NUMBER = \"350996086543\"\n",
    "ROLE_NAME = \"notebookAccessRole\"\n",
    "ROLE_ARN = f\"arn:aws:iam::{ACCOUNT_NUMBER}:role/{ROLE_NAME}\"\n",
    "SOURCE_BUCKET = \"impact-datashare\"\n",
    "DESTINATION_BUCKET = f\"s3://{ACCOUNT_NUMBER}-model-bucket\"\n",
    "\n",
    "# NOTE: Use image_url function above to create a valid url, if the shapefile generation was not done in Aqua, TrueColor \n",
    "DATA_FOLDER = \"data\"\n",
    "EVENT = \"hld-labeled\"\n",
    "IMAGE_FOLDER = \"images\"\n",
    "SHAPEFILE_FOLDER = \"shapefiles\"\n",
    "URL = \"https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi?SERVICE=WMS&REQUEST=GetMap&layers=MODIS_Aqua_CorrectedReflectance_TrueColor&version=1.3.0&crs=EPSG:4326&transparent=false&width={}&height={}&bbox={}&format=image/tiff&time={}\"\n",
    "KM_PER_DEG_AT_EQ = 111.\n",
    "RESOLUTION = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment for data transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assumed_role_session():\n",
    "    # Assume the \"notebookAccessRole\" role we created using AWS CDK.\n",
    "    client = boto3.client('sts')\n",
    "    creds = client.assume_role(\n",
    "        RoleArn=ROLE_ARN,\n",
    "        RoleSessionName=ROLE_NAME\n",
    "    )['Credentials']\n",
    "    return boto3.session.Session(\n",
    "        aws_access_key_id=creds['AccessKeyId'],\n",
    "        aws_secret_access_key=creds['SecretAccessKey'],\n",
    "        aws_session_token=creds['SessionToken'],\n",
    "        region_name='us-east-1'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods to download and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(foldername):\n",
    "    if os.path.exists(foldername):\n",
    "        print(f\"'{foldername}' folder already exists.\")\n",
    "        return\n",
    "    os.makedirs(foldername)\n",
    "    print(f\"Created folder: {foldername}\")\n",
    "\n",
    "    \n",
    "def delete_folder(foldername):\n",
    "    shutil.rmtree(foldername) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove already existing folder for a split and create a new one with passed filenames\n",
    "def create_split(split, files):\n",
    "    \"\"\"\n",
    "    Clear and create folder with new files.\n",
    "    split: choice of \"train\", \"test\", and \"val\"\n",
    "    files: list of tiff file paths\n",
    "    \"\"\"\n",
    "    print(f'Preparing {split} split with {len(files)} examples.')\n",
    "    folder_name = f\"{DATA_FOLDER}/{split}\"\n",
    "    if os.path.exists(folder_name):\n",
    "        delete_folder(folder_name)\n",
    "    mkdir(folder_name)\n",
    "    for filename in files:\n",
    "        internal_filename = filename.split('/')[-1]\n",
    "        bitmap_filename = filename.replace('.tiff', '_bitmap.png')\n",
    "        shutil.copyfile(filename, f\"{folder_name}/{internal_filename}\")\n",
    "        shutil.copyfile(bitmap_filename, f\"{folder_name}/{bitmap_filename.split('/')[-1]}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train, val, and test splits\n",
    "def prepare_splits(source_folder, splits={'train': 0.6, 'val': 0.2, 'test': 0.2}):\n",
    "    files = glob(f\"{source_folder}/*.tiff\")\n",
    "    print(f\"Total examples found: {len(files)}\")\n",
    "    random.shuffle(files)\n",
    "    length = len(files)\n",
    "    train_limit = math.ceil(length * splits['train'])\n",
    "    val_limit = train_limit + math.ceil(length * splits['train'])\n",
    "    create_split('train', files[0:train_limit])\n",
    "    create_split('val', files[train_limit:val_limit])\n",
    "    create_split('test', files[train_limit:val_limit])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = assumed_role_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point='hld_sagemaker_demo.py',\n",
    "    source_dir=\"/home/ec2-user/SageMaker/workshop_notebooks/chapter-3/src\",\n",
    "    role=ROLE_NAME,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    py_version='py3',\n",
    "    output_path=DESTINATION_BUCKET,\n",
    "    image_uri='763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.4.1-gpu-py37-cu110-ubuntu18.04',\n",
    "    distribution={\n",
    "        'parameter_server': {'enabled': True}\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "train_images = sagemaker_session.upload_data(path='../data/train')\n",
    "val_images = sagemaker_session.upload_data(path='../data/val')\n",
    "test_images = sagemaker_session.upload_data(path='../data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({'train': train_images, 'eval': val_images, 'test': test_images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.deploy(initial_instance_count=1, instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment (move to chapter-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "model = TensorFlowModel(framework_version='2.4.1', model_data=f'{DESTINATION_BUCKET}/tensorflow-training-2021-05-05-10-10-34-979/output/model.tar.gz', role=ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.t2.large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(np.asarray([np.zeros((256, 256, 3))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
